{"meta":{"title":"码猿不正经","subtitle":"","description":"walims的个人博客","author":"walims","url":"https://walims.xyz","root":"/"},"pages":[{"title":"","date":"2024-06-27T13:31:59.379Z","updated":"2024-06-27T13:31:59.379Z","comments":true,"path":"about/index.html","permalink":"https://walims.xyz/about/index.html","excerpt":"","text":""},{"title":"所有标签","date":"2024-06-27T13:35:41.055Z","updated":"2024-06-27T13:35:41.055Z","comments":true,"path":"tags/index.html","permalink":"https://walims.xyz/tags/index.html","excerpt":"","text":""},{"title":"所有分类","date":"2024-06-27T13:27:47.937Z","updated":"2024-06-27T13:27:47.937Z","comments":true,"path":"categories/index.html","permalink":"https://walims.xyz/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"Spark面试题一--Spark base","slug":"面试题/Spark面试题一--Spark base","date":"2024-07-06T13:52:33.970Z","updated":"2024-07-07T13:28:31.787Z","comments":true,"path":"2024/07/06/面试题/Spark面试题一--Spark base/","permalink":"https://walims.xyz/2024/07/06/%E9%9D%A2%E8%AF%95%E9%A2%98/Spark%E9%9D%A2%E8%AF%95%E9%A2%98%E4%B8%80--Spark%20base/","excerpt":"","text":"1.阐述下对Spark的并行度理解​ Spark作业中 ，各个stage的task的数量 ，代表Spark作业在各个阶段stage的并行度。 分为资源并行度(物理并行度)和数据并行度(逻辑并行度) 资源并行度：由节点数（executor）和cpu数（core）决定的 数据并行度：task的数量，partition大小 task又分为map时的task和reduce（shuffle）时的task; task的数目和很多因素有关：资源的总core数，spark.default.parallelism参数spark.sql.shuffle.partitions参数，读取数据源的类型，shuffle方法的第二个参数，repartition的数目等等。 ​ 如果Task的数量多，能用的资源也多，那么并行度自然就好。如果Task的数据少，资源很多有一定的浪费，但是也还好。如果Task数目很多，但是资源少，那么会执行完一批，再执行下一批。所以官方给出的建议是，这个Task数目要是core总数的2-3倍为佳。如果core有多少Task就有多少那么有些比较快的task执行完了，一些资源就会处于等待的状态。 ​ 如何设置Task数量： ​ 理想情况下将Task数量设置成与Application总CPUCore数量相同（理想情况，150个core，分配150Task），官方推荐Task数量设置成Application总CPUCore数量的2-3倍（150个cpucore，设置task数量为300-500）。 ​ 与理想情况不同的是：有些Task会运行快一点，比如50s就完了，有些Task可能会慢一点，要一分半才运行完，所以如果你的Task数量刚好设置的跟CPUCore数量相同，也可能会导致资源的浪费，比如150Task，10个先运行完了，剩余140个还在运行，但是这个时候就有10个core会处于等待状态。 2.设置Application的并行度​ 参数spark.default.parallelism默认是没有值的，如果设置了值，是在shuffle的过程才会起作用 123new SparkConf().set(&quot;spark.default.parallelism&quot;,&quot;10&quot;)// rdd2的分区数就是10，rdd1的分区数不受这个参数的影响val rdd2 = rdd1.reduceByKey(_+_) 如何根据数据量（task数目）配置资源 ​ 当提交一个Spark Application时，设置资源信息如下，基本已经达到了集群或者yarn队列的资源上限： ​ task没有设置或者设置的很少，比如为100个task，平均分配一下，每个executor分配到2个task，每个executor剩下的一个CPU core就浪费掉了！ ​ 虽然分配充足了，但是问题是：并行度没有与资源相匹配，导致分配下去的资源都浪费掉了。合理的并行度的设置，应该要设置的足够大，大到可以完全合理的利用你的集群资源。可以调整task数目，按照原则：Task数量，设置成Application总CPU core数量的2～3倍 ​ 实际项目中，往往依据数据数量（task数目）配置资源 3.Spark有几种部署方式？请分别简要论述 Local: 运行在一台机器上 ，通常是练手或者测试环境。 Standalone: 构建一个基于Mster+Slaves的资源调度集群 ，Spark任务提交给Master运行。是Spark自身的一个调 度系统。 Yarn: Spark客户端直接连接Yarn，不需要额外构建Spark集群。有yarn-client和yarn-cluster两种模式，主要区别 在于： Driver程序的运行节点。 Mesos ：国内大环境比较少用。 Mesos是一个集群管理器，可以运行多个框架和应用，包括Spark。 它允许资源的精细控制和隔离，适合运行多个不同的分布式应用。 4.Spark任务使用什么进行提交，javaEE界面还是脚本 Spark任务使用shell脚本进行提交 5.Spark提交作业参数（重点） 在提交任务时的几个重要参数 12345executor-cores —— 每个executor使用的内核数 ，默认为1 ，官方建议2-5个 ，我们企业是4个 num-executors —— 启动executors的数量 ，默认为2executor-memory —— executor内存大小 ，默认1G driver-cores —— driver使用内核数 ，默认为1driver-memory —— driver内存大小 ，默认512M 例如，一个提交任务如下： 123456789101112# 如果这里通过--queue 指定了队列，那么可以免去写--masterspark-submit \\ --master local[5] \\ --driver-cores 2 \\ --driver-memory 8g \\ --executor-cores 4 \\ --num-executors 10 \\ --executor-memory 8g \\ --class PackageName.ClassName XXXX.jar \\ --name &quot;Spark Job Name&quot; \\ InputPath \\ OutputPath","categories":[{"name":"Spark面试题","slug":"Spark面试题","permalink":"https://walims.xyz/categories/Spark%E9%9D%A2%E8%AF%95%E9%A2%98/"}],"tags":[{"name":"面试题","slug":"面试题","permalink":"https://walims.xyz/tags/%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"name":"Spark","slug":"Spark","permalink":"https://walims.xyz/tags/Spark/"}]},{"title":"排序算法-快速排序","slug":"算法/排序算法-快速排序","date":"2024-07-02T13:36:31.652Z","updated":"2024-07-06T13:51:13.131Z","comments":true,"path":"2024/07/02/算法/排序算法-快速排序/","permalink":"https://walims.xyz/2024/07/02/%E7%AE%97%E6%B3%95/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95-%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8F/","excerpt":"","text":"快速排序​ 快速排序（Quick Sort）是一种高效的排序算法，由英国计算机科学家托尼·霍尔（Tony Hoare）在1960年提出。它采用分治法（Divide and Conquer）的策略来把一个序列分为较小和较大的两个子序列，然后递归地排序两个子序列。 快速排序的步骤： 选择基准值（Pivot）：从数组中选择一个元素作为基准值，通常选择第一个元素、最后一个元素、中间元素或随机元素。 分区操作（Partitioning）：重新排列数组，所有比基准值小的元素摆放在基准前面，所有比基准值大的元素摆在基准的后面，相同的数可以到任意一边。在这个分区退出之后，该基准就处于数组的中间位置。这个称为分区（partition）操作。 递归排序：递归地（Recursive）把小于基准值的子数组和大于基准值的子数组排序。 ​ 快速排序的性能在最坏的情况下是O(n^2)，但平均性能是O(n log n)，这使得它在实际应用中非常高效。它的空间复杂度是O(log n)，这是因为递归调用的栈空间。 python实现快速排序123456789101112131415161718192021222324252627def quick_sort(arr): # 快速排序的辅助函数，用于递归排序 def _quick_sort(items, low, high): if low &lt; high: # pi是分区索引，基准值最终的位置 pi = partition(items, low, high) # 分别对基准值左边和右边的子数组进行快速排序 _quick_sort(items, low, pi - 1) _quick_sort(items, pi + 1, high) # 执行分区操作的函数 def partition(items, low, high): # 选择基准值，这里选择数组的最后一个元素 pivot = items[high] i = low - 1 # `i`指向比基准小的区域的最后一个元素 for j in range(low, high): # 如果当前元素小于或等于基准值 if items[j] &lt;= pivot: i += 1 # 交换元素，将`i`和`j`指向的元素交换 items[i], items[j] = items[j], items[i] # 交换基准值到它最终的位置 items[i + 1], items[high] = items[high], items[i + 1] return i + 1 # 调用辅助函数进行排序 _quick_sort(arr, 0, len(arr) - 1) ​ 还可以使用列表生成式创建子列表来实现，这种方法代码简单，但创建了额外的列表，占用内存空间，性能较低。 123456789# 列表生成式实现快速排序def quicksort(arr): if len(arr) &lt;= 1: return arr pivot = arr[len(arr) // 2] left = [x for x in arr if x &lt; pivot] middle = [x for x in arr if x == pivot] right = [x for x in arr if x &gt; pivot] return quicksort(left) + middle + quicksort(right)","categories":[{"name":"算法","slug":"算法","permalink":"https://walims.xyz/categories/%E7%AE%97%E6%B3%95/"},{"name":"排序","slug":"算法/排序","permalink":"https://walims.xyz/categories/%E7%AE%97%E6%B3%95/%E6%8E%92%E5%BA%8F/"}],"tags":[{"name":"算法","slug":"算法","permalink":"https://walims.xyz/tags/%E7%AE%97%E6%B3%95/"},{"name":"排序","slug":"排序","permalink":"https://walims.xyz/tags/%E6%8E%92%E5%BA%8F/"}]},{"title":"算法入门-时间复杂度与空间复杂度","slug":"算法/算法入门-时间复杂度与空间复杂度","date":"2024-06-29T13:50:04.904Z","updated":"2024-07-01T12:52:13.503Z","comments":true,"path":"2024/06/29/算法/算法入门-时间复杂度与空间复杂度/","permalink":"https://walims.xyz/2024/06/29/%E7%AE%97%E6%B3%95/%E7%AE%97%E6%B3%95%E5%85%A5%E9%97%A8-%E6%97%B6%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A6%E4%B8%8E%E7%A9%BA%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A6/","excerpt":"","text":"​ 算法复杂度是衡量算法性能的一个重要指标，它描述了算法执行所需时间和资源（通常是计算步骤数或内存使用量）与输入数据规模的关系。算法复杂度通常用大O符号（Big O notation）来表示，最常见的是时间复杂度和空间复杂度。 时间复杂度​ 算法的时间复杂度是指算法在执行时所需的时间与输入数据规模之间的关系。它是衡量算法效率的一个重要标准。 常见的时间复杂度类别 常数时间复杂度 - O(1)：算法的执行时间与输入数据的规模无关。 123# 示例代码 访问数组的特定索引def access_element(arr, index): return arr[index] 线性时间复杂度 - O(n)：算法的执行时间与输入数据的规模成正比。 123456# 示例代码 遍历数组def sum_array(arr): total = 0 for num in arr: total += num return total 平方时间复杂度 - O(n^2)：算法的执行时间与输入数据规模的平方成正比。 1234567# 示例代码 双层循环def sum_of_pairs(arr): total = 0 for i in range(len(arr)): for j in range(i, len(arr)): total += arr[j] return total 对数时间复杂度 - O(log n)：算法的执行时间与输入数据规模的对数成正比。 123456789101112# 示例代码 二分查找def binary_search(arr, target): left, right = 0, len(arr) - 1 while left &lt;= right: mid = (left + right) // 2 if arr[mid] == target: return mid elif arr[mid] &lt; target: left = mid + 1 else: right = mid - 1 return -1 线性对数时间复杂度 - O(n log n)：常见于排序算法，如快速排序和归并排序。 123456789# 示例代码 快速排序def quicksort(arr): if len(arr) &lt;= 1: return arr pivot = arr[len(arr) // 2] left = [x for x in arr if x &lt; pivot] middle = [x for x in arr if x == pivot] right = [x for x in arr if x &gt; pivot] return quicksort(left) + middle + quicksort(right) 指数时间复杂度 - O(2^n)：算法的执行时间与输入数据规模的指数成正比。 123456# 示例代码 暴力搜索def brute_force_search(options, target): for option in options: if option == target: return True return False 时间复杂度通常关注最坏情况下的性能，但也有时会考虑平均情况或最佳情况的时间复杂度。 空间复杂度​ 算法的空间复杂度是指算法在执行过程中所需的存储空间量与输入数据规模之间的关系。它是用来衡量算法在内存使用效率上的一个重要指标。 常见的空间复杂度类别 O(1) - 常数空间复杂度：算法所需的存储空间不随输入规模的变化而变化。 1234567# 示例：使用一个固定大小的变量来存储结果。def max_value_in_list(lst): max_val = lst[0] for num in lst: if num &gt; max_val: max_val = num return max_val O(n) - 线性空间复杂度：算法所需的存储空间与输入数据的规模成正比。 123# 示例：创建一个与输入列表同样大小的新列表。def create_copy(lst): return [x for x in lst] **O(n^2) - 平方空间复杂度:**算法所需的存储空间与输入数据规模的平方成正比。 123# 示例：创建一个二维列表，其行数和列数都与输入数据规模有关。def create_matrix(n): return [[0 for _ in range(n)] for _ in range(n)] **O(log n) - 对数空间复杂度:**算法所需的存储空间与输入数据规模的对数成正比。 123456789101112# 示例：对数空间通常与递归算法相关，如递归二分查找。def binary_search(arr, target): left, right = 0, len(arr) - 1 while left &lt;= right: mid = (left + right) // 2 if arr[mid] == target: return True elif arr[mid] &lt; target: left = mid + 1 else: right = mid - 1 return False **O(k) - 多项式空间复杂度:**算法所需的存储空间与输入数据规模的多项式成正比，其中k是多项式的度数。 123# 示例：创建一个大小为n^k的列表，其中k是一个常数。def create_high_degree_space(n, k): return [[0 for _ in range(n)] for _ in range(n**k)] **O(2^n) - 指数空间复杂度:**算法所需的存储空间与输入数据规模的指数成正比。 12345# 示例：使用递归创建所有可能的二进制字符串。def all_binary_strings(n): if n == 0: return [&#x27;&#x27;] return all_binary_strings(n-1) + [&#x27;0&#x27; + s for s in all_binary_strings(n-1)] + [&#x27;1&#x27; + s for s in all_binary_strings(n-1)] ​ 在分析空间复杂度时，通常只考虑额外使用的存储空间，而不包括输入数据本身占用的空间。此外，空间复杂度的分析也关注最坏情况下的空间使用，但在某些情况下，平均情况或最佳情况的空间复杂度也是值得考虑的。 ​ 在实际应用中，算法的选择往往需要在时间复杂度和空间复杂度之间做出权衡。","categories":[{"name":"算法","slug":"算法","permalink":"https://walims.xyz/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"算法","slug":"算法","permalink":"https://walims.xyz/tags/%E7%AE%97%E6%B3%95/"}]},{"title":"Hadoop面试题之Hadoop优化","slug":"面试题/Hadoop面试题之Hadoop优化","date":"2024-06-27T11:40:59.000Z","updated":"2024-07-04T06:32:24.582Z","comments":true,"path":"2024/06/27/面试题/Hadoop面试题之Hadoop优化/","permalink":"https://walims.xyz/2024/06/27/%E9%9D%A2%E8%AF%95%E9%A2%98/Hadoop%E9%9D%A2%E8%AF%95%E9%A2%98%E4%B9%8BHadoop%E4%BC%98%E5%8C%96/","excerpt":"","text":"1.MapReduce跑得慢的原因Mapreduce程序效率的瓶颈在于两点： 计算机性能：CPU、内存、磁盘健康状况以及网络速度都可能影响MapReduce任务的执行效率。 I&#x2F;O 操作优化： 数据倾斜:某些任务由于数据分布不均匀，导致部分任务处理数据量远大于其他任务。 map和reduce数设置不合理:太多或太少的任务可能导致资源竞争或等待时间延长。 reduce等待过久:Map任务完成后Reduce任务才开始执行，如果Map任务执行时间过长，将导致Reduce等待。 小文件过多:HDFS上的小文件过多会增加NameNode的内存负担，并可能导致大量的Map任务。 大量的不可分块的超大文件:大文件如果不可分块，将不能有效利用集群的并行处理能力。 spill次数过多:Map阶段数据处理时，内存缓冲区溢出到磁盘的次数过多，增加了I&#x2F;O操作。 merge次数过多:Map任务结束前，需要将溢写到磁盘的多个文件合并，如果合并次数过多，也会增加处理时间。 2.MapReduce优化方法 数据输入优化： 合并小文件：在执行mr任务前将小文件进行合并，大量的小文件会产生大量的map任务，增大map任务装载次数，而任务的装载比较耗时，从而导致mr运行较慢。 采用ConbinFilelnputFormat来作为输入，它可以将多个文件合并成一个单独的 split，解决输入端大量小文件场景。 Map阶段优化： 减少spill次数：通过调整io.sort.mb及sort.spill.percent参数值，增大触发spil的内存上限，减少spill次数,从而减少磁盘 I&#x2F;O。 减少merge次数：通过调整io.sort.factor参数，增大merge的文件数目，减少merge的次数，从而缩短mr处理时间。 在map之后先进行combine处理，减少I&#x2F;O。 Reduce阶段优化： 合理设置map和reduce数：两个都不能设置太少，也不能设置太多。太少，会导致task等待，延长处理时间；太多，会导致map、reduce任务间竞争资源，造成处理超时等错误。 设置map、reduce共存：调整slowstart.completedmaps参数，使map运行到一定程度后，reduce也开始运行，减少reduce的等待时间。 规避使用reduce以减少网络消耗，因为Reduce在用于连接数据集的时候将会产生大量的网络消耗。 合理设置reduc端的buffer ，默认情况下 ，数据达到一个阈值的时候 ，buffer中的数据就会写入磁盘 ，然后reduce会从磁盘中获得所有的数据。也就是说 ，buffer和reduce是没有直接关联的 ，中间多个一个写磁盘-&gt;读磁盘的过程 ，既然有这个弊端 ，那么就可以通过参数来配置 ，使得buffer中的一部分数据可以直接输送到reduce ，从而减少IO开销 ：mapred.job.reduce.input.buffer.percent ，默认为0.0。当值大于0的时候 ，会保留指定比例的内存读buffer中的数据直接拿给reduce使用。这样一来 ，设置buffer需要内存 ，读取数据需要内存 ，reduce计算也要内存 ，所以要根据作业的运行情况进行调整。 IO传输优化： 采用数据压缩的方式，减少网络IO的的时间。安装Snappy和LZOP压缩编码器。 使用二进制文件格式如 SequenceFile 来提高 IO 效率。 数据倾斜问题： 数据倾斜现象： 数据频率倾斜——某一个区域的数据量要远远大于其他区域。 数据大小倾斜——部分记录的大小远远大于平均值。 如何收集倾斜数据： 在reduce方法中加入记录map输出键的详细情况的功能： 1234567891011121314151617public static final String MAX_VALUES = &quot;skew.maxvalues&quot;;private int maxValueThreshold;@Overridepublic void configure(JobConfjob) &#123; maxValueThreshold =job.getInt(MAX_VALUES, 100);&#125; @Overridepublic void reduce(Text key, Iterator&lt;Text&gt; values,OutputCollector&lt;Text, Text&gt; output,Reporter reporter) throws IOException &#123; inti = 0; while (values.hasNext()) &#123; values.next(); i++; &#125; if (++i &gt; maxValueThreshold)&#123; log.info(&quot;Received &quot; + i + &quot; values for key &quot; + key); &#125;&#125; 减少数据倾斜的方法： 方法1：抽样和范围分区 ​ 可以通过对原始数据进行抽样得到的结果集来预设分区边界值。 方法2：自定义分区 ​ 另一个抽样和范围分区的替代方案是基于输出键的背景知识进行自定义分区。例如，如果map输出键的单词来源于一本书。其中大部分必然是省略词（stopword）。那么就可以将自定义分区将这部分省略词发送给固定的-部分reduce实例。而将其他的都发送给剩余的reduce实例。 方法3：Combine ​ 使用Combine可以大量地减小数据频率倾斜和数据大小倾斜。在可能的情况下，Combine的目的就是聚合并精简数据。 常用的调优参数： 资源相关参数 以下参数是在用户自己的mr应用程序中配置就可以生效（mapred-default.xml） 配置参数 参数说明 mapreduce.map.memory.mb 一个Map Task可使用的资源上限（单位:MB），默认为1024。 如果Map Task实际使用的资源量超过该值 ，则会被强制杀死。 mapreduce.reduce.memory.mb 一个Reduce Task可使用的资源上限（单位:MB），默认为1024。如果Reduce Task实际使用的资源量超过该值，则会被强制杀死。 mapreduce.map.cpu.vcores 每个Map task可使用的最多cpu core数目 ，默认值: 1 mapreduce.reduce.cpu.vcores 每个Reduce task可使用的最多cpu core数目 ，默认值: 1 mapreduce.reduce.shuffle.parallelcopies 每个reduce去map中拿数据的并行数。默认值是5 mapreduce.reduce.shuffle.merge.percent buffer中的数据达到多少比例开始写入磁盘。默认值0.66 mapreduce.reduce.shuffle.input.buffer.percent buffer大小占reduce可用内存的比例。默认值0.7 mapreduce.reduce.input.buffer.percent 指定多少比例的内存用来存放buffer中的数据，默认值是0.0 应该在yarn启动之前就配置在服务器的配置文件中才能生效（yarn-default.xml） 配置参数 参数说明 yarn.scheduler.minimum-allocation-mb 1024 给应用程序container分配的最小内存 yarn.scheduler.maximum-allocation-mb 8192 给应用程序container分配的最大内存 yarn.scheduler.minimum-allocation-vcores 1 每个container申请的最小CPU核数 yarn.scheduler.maximum-allocation-vcores 32 每个container申请的最大CPU核数 yarn.nodemanager.resource.memory-mb 8192 给containers分配的最大物理内存 shuffle性能优化的关键参数 ，应在yarn启动之前就配置好（mapred-default.xml） 配置参数 参数说明 mapreduce.task.io.sort.mb 100 shuffle的环形缓冲区大小 ，默认100m mapreduce.map.sort.spill.percent 0.8 环形缓冲区溢出的阈值 ，默认80% 容错相关参数(mapreduce性能优化) 配置参数 参数说明 mapreduce.map.maxattempts 每个Map Task最大重试次数，一旦重试参数超过该值，则认为Map Task运行失败 ，默认值：4。 mapreduce.reduce.maxattempts 每个Reduce Task最大重试次数 ，一旦重试参数超过该值 ，则认为Map Task运行失败 ，默认值：4。 mapreduce.task.timeout Task超时时间 ，经常需要设置的一个参数 ，该参数表达的意思为：如果一个task在一定时间内没有任何进入 ，即不会读取新的数据， 也没有输出数据，则认为该task处于block状态，可能是卡住了，也 许永远会卡主 ，为了防止因为用户程序永远block住不退出 ，则强 制设置了一个该超时时间（单位毫秒），默认是600000。如果你 的程序对每条输入数据的处理时间过长（比如会访问数据库，通过 网络拉取数据等），建议将该参数调大，该参数过小常出现的错误提示是“AttemptID:attempt_14267829456721_123456_m_000224_0 Timed out after 300 secsContainer killed by the ApplicationMaster.”。 3.HDFS小文件弊端及优化方法 HDFS小文件弊端： ​ HDFS上每个文件都要在namenode上建立一个索引l，这个索引l的大小约为150byte，这样当小文件比较多的时候，就会产生很多的索引文件，一方面会大量占用namenode的内存空间，另一方面就是索引文件过大时的索引速度变慢。 解决方案： Hadoop Archive ​ 一个高效地将小文件放入HDFS块中的文件存档工具。使用 hadoop archive 命令将多个小文件打包成一个 HAR 文件，减少 NameNode 的内存使用并允许对文件进行透明访问，提高访问效率。 Sequence file ​ sequence file由一系列的二进制key&#x2F;value组成，如果key为文件名，value为文件内容，则可以将**大批小文件合并成一个大文件(SequenceFile)**。 CombineFilelnput Format ​ CombineFilelnputFormat是一种新的inputformat，使用它将多个文件合并成一个单独的split，同时它会考虑数据的存储位置，提高数据处理效率。 开启JVM重用 ​ 对于大量小文件Job，可以开启JVM重用,会减少45%运行时间。 ​ JVM重用理解：一个map运行一个jvm，重用的话，在一个map在jvm上运行完毕后，jvm继续运行其他jvm。 ​ 具体设置：mapreduce.job.jvm.numtasks值在10-20之间。 4.MapReduce怎么解决数据均衡问题，如何确定分区号？ ​ 数据均衡问题指的就是某个节点或者某几个节点的任务运行的比较慢，拖慢了整个Job的进度。实际上数据均衡问题就是数据倾斜问题 ，解决方案同解决数据倾斜的方案。（见上一题） ​ MapReduce中分区默认是按hashcode来分的 ，即根据key的hashCode对Reduce Tasks个数取模得到。 ​ 用户可以自定义分区类 ，需要继承系统的Partitioner类 ，重写getPartition()方法即可。 5. Hadoop中job和Tasks之间的区别是什么 ​ 编写好的一个程序 ，我们称为Mapreduce程序 ，一个Mapreduce程序就是一个Job ，而一个Job里面可以有一个或多个Task ，Task又可以区分为Map Task和Reduce Task.","categories":[{"name":"Hadoop面试题","slug":"Hadoop面试题","permalink":"https://walims.xyz/categories/Hadoop%E9%9D%A2%E8%AF%95%E9%A2%98/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://walims.xyz/tags/Hadoop/"},{"name":"面试题","slug":"面试题","permalink":"https://walims.xyz/tags/%E9%9D%A2%E8%AF%95%E9%A2%98/"}]},{"title":"Hadoop面试题之Yarn","slug":"面试题/Hadoop面试题之Yarn","date":"2024-06-27T11:40:55.000Z","updated":"2024-06-27T13:37:52.309Z","comments":true,"path":"2024/06/27/面试题/Hadoop面试题之Yarn/","permalink":"https://walims.xyz/2024/06/27/%E9%9D%A2%E8%AF%95%E9%A2%98/Hadoop%E9%9D%A2%E8%AF%95%E9%A2%98%E4%B9%8BYarn/","excerpt":"","text":"1.简述Hadoop1与Hadoop2的架构异同 加入了yarn解决了资源调度的问题 加入了对zookeeper的支持实现比较可靠的高可用 2.为什么会产生yarn,它解决了什么问题，有什么优势？ 解决的问题：Yarn最主要的功能就是解决运行的用户程序与yarn框架完全解耦 优势：Yarn上可以运行各种类型的分布式运算程序（mapreduce只是其中的一种），比如mapreduce、storm程序，spark程序…. 3.HDFS的数据压缩算法有哪些？每种算法的优缺点和应用场景是什么？ gzip压缩 优点：压缩率比较高，而且压缩&#x2F;解压速度也比较快；hadoop本身支持，在应用中处理gzip格式的文件就和直接处理文本一样；大部分linux系统都自带gzip命令 ，使用方便。 缺点：不支持split。 应用场景：当每个文件压缩之后在130M以内的（1个块大小内），都可以考虑用gzip压缩格式。例如说一天或者一个小时的日志压缩成一个gzip文件 ，运行mapreduce程序的时候通过多个gzip文件达到并发。hive程序， streaming程序 ，和java写的mapreduce程序完全和文本处理一样 ，压缩之后原来的程序不需要做任何修改。 Bzip2压缩 优点：支持split；具有很高的压缩率，比gzip压缩率都高；hadoop本身支持，但不支持native；在linux系统下自带bzip2命令 ，使用方便。 缺点：压缩&#x2F;解压速度慢；不支持native。 应用场景：适合对速度要求不高 ，但需要较高的压缩率的时候 ，可以作为mapreduce作业的输出格式；或者输出之后的数据比较大，处理之后的数据需要压缩存档减少磁盘空间并且以后数据用得比较少的情况；或者对单个很大的文本文件想压缩减少存储空间 ，同时又需要支持split ，而且兼容之前的应用程序（即应用程序不需要修改）的情况。 Lzo压缩 优点：压缩&#x2F;解压速度也比较快，合理的压缩率；支持split，是hadoop中最流行的压缩格式；可以在linux系统下安装lzop命令 ，使用方便。 缺点：压缩率比gzip要低一些；hadoop本身不支持，需要安装；在应用中对lzo格式的文件需要做一些特殊处理（为了支持split需要建索引 ，还需要指定inputformat为lzo格式）。 应用场景：一个很大的文本文件，压缩之后还大于200M以上的可以考虑，而且单个文件越大，lzo优点越明显。 Snappy压缩 优点：高速压缩速度和合理的压缩率。 缺点：不支持split；压缩率比gzip要低；hadoop本身不支持 ，需要安装； 应用场景：当Mapreduce作业的Map输出的数据比较大的时候 ，作为Map到Reduce的中间数据的压缩格式；或者作为一个Mapreduce作业的输出和另外一个Mapreduce作业的输入 4.Hadoop的调度器总结 目前，Hadoop的作业调度器主要有三种：FIFO Scheduler、Capacity Scheduler和Fair Scheduler Hadoop默认的资源调度器是FIFO Scheduler，Hadoop2.7.2默认的资源调度器是Capacity Scheduler 具体设置详见：yarn-default.xml文件: 1234567&lt;property&gt;&lt;description&gt;The class to use as the resource scheduler.&lt;/description&gt;&lt;name&gt;yarn.resourcemanager.scheduler.class&lt;/name&gt;&lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler&lt;/value&gt;&lt;/property&gt; 先进先出调度器（FIFO Scheduler） ​ Hadoop中默认的调度器，是Hadoop最早实现的调度器之一，也是最简单的一种。它按照任务到达的顺序来进行调度，即先提交的任务先被执行。虽然实现简单，但它并不适合用于生产环境，因为不考虑作业的优先级、资源需求和执行时间等因素，可能导致资源利用不高，长作业等待时间过长。 容量调度器（Capacity Scheduler） Hadoop2.x之后默认的调度器 采用多队列的方式。以队列为单位划分资源，每个队列可设定一定比例的资源最低保证和使用上限。每个队列内部采用FIFO的调度策略。 调度过程：当job提交后大致由如下过程1）为job选择队列，通过计算队列资源使用率，选择”最闲的”队列2）将队列中的job按照提交时间排序，选择最早的Application分配资源 一个队列中同时只能有一个job执行，队列并行度等于队列的个数，允许并行运行 公平调度器（ Fair Scheduler） ​ 公平调度器也是Hadoop YARN引入的调度器，它的主要目标是保证所有作业公平地共享集群资源。它根据作业的需求和历史执行情况来动态地分配资源。不同于容量调度器的静态资源划分，公平调度器会在运行时根据资源需求进行动态调整。每个作业被分配的资源量与其他作业的需求和当前集群的负载情况成比例。这使得长作业无需等待过长时间，同时短作业也能得到及时的响应。","categories":[{"name":"Hadoop面试题","slug":"Hadoop面试题","permalink":"https://walims.xyz/categories/Hadoop%E9%9D%A2%E8%AF%95%E9%A2%98/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://walims.xyz/tags/Hadoop/"},{"name":"面试题","slug":"面试题","permalink":"https://walims.xyz/tags/%E9%9D%A2%E8%AF%95%E9%A2%98/"}]},{"title":"Hadoop面试题之MapReduce","slug":"面试题/Hadoop面试题之MapReduce","date":"2024-06-27T11:40:51.000Z","updated":"2024-06-27T13:37:47.237Z","comments":true,"path":"2024/06/27/面试题/Hadoop面试题之MapReduce/","permalink":"https://walims.xyz/2024/06/27/%E9%9D%A2%E8%AF%95%E9%A2%98/Hadoop%E9%9D%A2%E8%AF%95%E9%A2%98%E4%B9%8BMapReduce/","excerpt":"","text":"1. 谈谈Hadoop序列化和反序列化及自定义bean对象实现序列化？ https://blog.csdn.net/klionl/article/details/105395340 序列化概述 什么是序列化和反序列化： 序列化就是把内存中的对象，转换成字节序列(或其他数据传输协议)以便于存储到磁盘(持久化)和网络传输。 反序列化就是将收到字节序列(或其他数据传输协议)或者是磁盘的持久化数据，转换成内存中的对象。 为什么要序列化 一般来说，“活的” 对象只生存在内存里，关机断电就没有了。而且“活的”对象只能由本地的进程使用，不能被发送到网络上的另外一台计算机。然而序列化可以存储“活的”对象，可以将“活的”对象发送到远程计算机。 为什么不用java的序列化： Java的序列化是一个重量级序列化框架（Serializable），一个对象被序列化后 ，会附带很多额外的信息 （各种校验信息 ，header，继承体系等），不便于在网络中高效传输。所以 ，hadoop自己开发了一套序列化机制（Writable），精简、高效。 Hadoop序列化特点: 紧凑: 高效使用存储空间。 快速: 读写数据的额外开销小。 可扩展: 随着通信协议的升级而可升级 互操作: 支持多语言的交互 自定义序列化接口（Writable） ​ 在企业开发中往往常用的基本序列化类型不能满足所有需求，比如在Hadoop框架内部传递一个bean对象，那么该对象就需要实现序列化接口。具体实现bean对象序列化步骤如下7步: 必须实现Writable接口 反序列化时，需要反射调用空参构造函数，所以必须有空参构造 123&gt;public FlowBean() &#123;super();&gt;&#125; 重写序列化方法 123456&gt;@Override&gt;public void write(DataOutput out) throws IOException &#123;out.writeLong(upFlow);out.writeLong(downFlow);out.writeLong(sumFlow);&gt;&#125; 重写反序列化方法 123456&gt;@Override&gt;public void readFields(DataInput in) throws IOException &#123;upFlow = in.readLong();downFlow = in.readLong();sumFlow = in.readLong();&gt;&#125; 注意反序列化的顺序和序列化的顺序完全一致 要想把结果显示在文件中，需要重写toString()，可用”\\t”分开，方便后续用。 1234&gt;@Override&gt;public string toString()&#123; return upFlow + &quot;\\t&quot; + downFlow + &quot;\\t&quot; + sumFlow;&gt;&#125; 如果需要将自定义的bean放在key中传输，则还需要实现Comparable接口，因为MapReduce框架中的Shuffle过程要求对key必须能排序。 12345&gt;@Override&gt;public int compareTo(FlowBean o) &#123;// 倒序排列，从大到小return this.sumFlow &gt; o.getSumFlow() ? -1 : 1;&gt;&#125; 2.FileInputFormat切片机制切片机制 简单地按照文件的内容长度进行切片 切片大小默认等于block大小（block默认128M） 切片时不考虑数据集整体，而是逐个针对每一个文件单独切片 具体步骤 (1) 根据driver中设置的输入目录，找到数据存储的目录。listStatus()。 (2) 开始遍历目录下的每一个文件，针对每个文件单独切片，不考虑数据整体。 (3) 遍历第一个文件xx.txt ​ a) 获取文件大小fs.getLen(xx.txt)。 ​ b) 默认情况下，切片大小=blocksize。 ​ c) 开始切片，例如有个300M的文件，形成第一个切片：word.bxt一D:128M 第2个切片：word.txt一128:256M 第3个切片：word.txt一256M:300M(每次切片时，都要判断切完剩下的部分是否大于块的1.1倍，不大于1.1倍就划分一块切片（如大于128m但小于140m）)。 ​ d) 将切片信息写到一个切片规划文件中Job.split。 ​ e) 数据切片只是在逻辑上对输入数据进行分片，并不会在磁盘上将其切分成分片进行存储。使用inputSplit只记录了分片的元数据信息，比如起始位置、长度以及所在的节点列表等。 ​ f) 注意：block是HDFS物理上存储的数据，切片是对数据逻辑上的划分。 (4) 提交切片规划文件到YARN上，YARN上的MrAppMaster就可以根据切片规划文件计算开启MapTask的个数。 3.自定义InputFormat流程 （1）自定义一个类继承FileInputFormat （2）改写RecordReader ，实现一次读取一个完整文件封装为KV 4.如何决定一个job的map和reduce的数量? 1）map数量 splitSize&#x3D;max{minSize,min{maxSize,blockSize}} map数量由处理的数据分成的block数量决定default_num &#x3D; total_size &#x2F; split_size; 2）reduce数量 reduce的数量job.setNumReduceTasks(x);x 为reduce的数量。不设置的话默认为 1。 5.MapTask的个数由什么决定 一个job的map阶段MapTask并行度（个数），由客户端提交job时的切片个数决定。 6.MapTask工作机制 read阶段：Map Task通过用户编写的RecordReader ，从输入InputSplit中解析出一个个key&#x2F;value map阶段：将解析出的key-value交给用户编写的map()函数进行处理，生成新的key-value collect阶段：将map的数据写到环形缓冲区（分区）中 spill溢写阶段：环形缓冲区数据满80%后溢写磁盘，生成临时文件，溢写之前需要进行排序 combine阶段：合并所有临时文件（而不是执行Combiner业务逻辑）：归并排序，将一些多次产生的小文件进行合并，形成一个大文件 详细步骤： （1）Read阶段 ：Map Task通过用户编写的RecordReader ，从输入InputSplit中解析出一个个key&#x2F;value。 （2）Map阶段：该节点主要是将解析出的key&#x2F;value交给用户编写map()函数处理 ，并产生一系列新的key&#x2F;value。 （3）Collect收集阶段：在用户编写map()函数中，当数据处理完成后，一般会调用OutputCollector.collect()输出结果。在该函数内部 ，它会将生成的key&#x2F;value分区（调用Partitioner） ，并写入一个环形内存缓冲区中。 （4）Spill阶段 ：即“溢写” ，当环形缓冲区满后 ，MapReduce会将数据写到本地磁盘上 ，生成一个临时文件。需要注意的是 ，将数据写入本地磁盘之前 ，先要对数据进行一次本地排序 ，并在必要时对数据进行合并、压缩等操作。 1234567&gt;溢写阶段详情：&gt;步骤1：利用快速排序算法对缓存区内的数据进行排序，排序方式是，先按照分区编号partition进行排序，然后按照&gt;key进行排序。这样 ，经过排序后 ，数据以分区为单位聚集在一起 ，且同一分区内所有数据按照key有序。&gt;步骤2：按照分区编号由小到大依次将每个分区中的数据写入任务工作目录下的临时文件output/spillN.out（N表示&gt;当前溢写次数）中。如果用户设置了Combiner ，则写入文件之前 ，对每个分区中的数据进行一次聚集操作。&gt;步骤3：将分区数据的元信息写到内存索引数据结构SpillRecord中 ，其中每个分区的元信息包括在临时文件中的偏&gt;移量、压缩前数据大小和压缩后数据大小。如果当前内存索引大小超过1MB ，则将内存索引写到文件output/spillN.out.index中。 （5）Combine阶段 ：当所有数据处理完成后 ，MapTask对所有临时文件进行一次合并 ，以确保最终只会生成一个 数据文件。 ​ 当所有数据处理完后 ，MapTask会将所有临时文件合并成一个大文件 ，并保存到文件output&#x2F;file.out中 ，同时生成相应的索引文件output&#x2F;file.out.index。 ​ 在进行文件合并过程中 ，MapTask以分区为单位进行合并。对于某个分区 ，它将采用多轮递归合并的方式。每轮合 并io.sort.factor（默认100）个文件，并将产生的文件重新加入待合并列表中，对文件排序后，重复以上过程，直到最终得到一个大文件。 ​ 让每个MapTask最终只生成一个数据文件，可避免同时打开大量文件和同时读取大量小文件产生的随机读取带来的开销。 7.ReduceTask工作机制 （1）Copy阶段：ReduceTask从各个MapTask上远程拷贝一片数据，并针对某一片数据，如果其大小超过一定阈值，则写到磁盘上，否则直接放到内存中。 （2）Merge阶段：在远程拷数据的同时，ReduceTask启动了两个后台线程对内存和磁盘上的文件进行合并，以防止内存使用过多或磁盘上文件过多。 （3）Sort阶段：按照MapReduce语义，用户编写reduce()函数输入数据是按key进行聚集的一组数据。为了将key相同的数据聚在一起，Hadoop采用了基于排序的策略。由于各个MapTask已经实现对自己的处理结果进行了局部排序，因此，ReduceTask只需对所有数据进行一次归并排序即可。 （4）Reduce阶段：reduce()函数将计算结果写到HDFS上。 8.请描述mapReduce有几种排序及排序发生的阶段排序的分类 部分排序 ​ MapReduce根据输入记录的键对数据集排序。保证输出的每个文件内部排序。 全排序 ​ 用Hadoop产生一个全局排序文件最简单的方法是使用一个分区。但该方法在处理大型文件时效率极低，因为一台机器必须处理所有输出文件，从而完全丧失了MapReduce所提供的并行架构。 ​ 替代方案：首先创建一系列排好序的文件；其次，串联这些文件；最后，生成一个全局排序的文件。主要思路时使用一个分区来描述输出的全局排序。例如：可以为待分析文件创建3个分区 ，在第一分区中 ，记录的单词首字母a-g ，第二分区记录单词首字母h-n, 第三分区记录单词首字母o-z。 辅助排序(GroupingComparator分组) ​ Mapreduce框架在记录到达reduce之前按键对记录排序，但键所对应的值并没有被排序。甚至在不同的执行轮次中 ，这些值的排序也不固定 ，因为它们来自不同的map任务且这些map任务在不同轮次中完成时间各不相同。一般来说 ，大多数MapReduce程序会避免让reduce函数依赖于值的排序。但是 ，有时也需要通过特定的方法对键进行排序和分组等以实现对值的排序。 二次排序 ​ 在自定义排序过程中 ，如果compareTo中的判断条件为两个即为二次排序。 排序发生的阶段map阶段和reduce阶段的shuffle阶段都会发生一次 一个是在Map side，发生在spill后，partition前 一个是在Reduce side, 发生在copy后，reduce前 自定义排序WritableComparable bean对象实现WritableComparable接口重写compareTo方法 ，就可以实现排序 12345@Overridepublic int compareTo(FlowBean o) &#123; // 倒序排列 ，从大到小 .sumFlow &gt; o.getSumFlow() ? -1 : 1;&#125; 9.请描述mapReduce中shuffle阶段的工作流程，如何优化shuffle阶段? 工作流程：分区 ，排序 ，溢写 ，拷贝到对应reduce机器上 优化：增加combiner ，压缩溢写的文件。 10.请描述MapReduce中combiner的作用是什么，一般使用情景，哪些情况不需要，及和reduce的区别? Combiner的意义就是对每一个maptask的输出进行局部汇总，以减小网络传输量。 Combiner能够应用的前提是不能影响最终的业务逻辑，而且，Combiner的输出kv应该跟reducer的输入kv类型要对应起来。 Combiner和reducer的区别在于运行的位置： Combiner是在每一个maptask所在的节点运行； Reducer是接收全局所有Mapper的输出结果。 11.如果没有定义partitioner ，那数据在被送达reducer前是如何被分区的？如果没有自定义的 partitioning，则默认的 partition 算法。 先对每一条数据的键(key)进行哈希运算 然后，将得到的哈希值对 Reducer 的总数进行取模运算 ， 取模运算的结果决定了每个键值对应该发送到哪个 Reducer,也就是分区号 12.有可能使 Hadoop 任务输出到多个目录中么？如果可以 ，怎么做？可以输出到多个目录中 ，采用自定义OutputFormat 实现步骤： ​ （1）自定义outputformat ​ （2）改写recordwriter ，具体改写输出数据的方法write()","categories":[{"name":"Hadoop面试题","slug":"Hadoop面试题","permalink":"https://walims.xyz/categories/Hadoop%E9%9D%A2%E8%AF%95%E9%A2%98/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://walims.xyz/tags/Hadoop/"},{"name":"面试题","slug":"面试题","permalink":"https://walims.xyz/tags/%E9%9D%A2%E8%AF%95%E9%A2%98/"}]},{"title":"Hadoop面试题之HDFS","slug":"面试题/Hadoop面试题之HDFS","date":"2024-06-21T08:21:59.974Z","updated":"2024-06-30T00:45:08.411Z","comments":true,"path":"2024/06/21/面试题/Hadoop面试题之HDFS/","permalink":"https://walims.xyz/2024/06/21/%E9%9D%A2%E8%AF%95%E9%A2%98/Hadoop%E9%9D%A2%E8%AF%95%E9%A2%98%E4%B9%8BHDFS/","excerpt":"","text":"1.HDFS的存储机制（读写流程）总体： 按块（block）存储，默认按照128M大小进行文件数据拆分，将不同拆分的块数据存储在不同datanode服务器上 三副本机制：为了保证HDFS的数据的安全性，避免数据丢失，HDFS对每个块数据进行备份，默认情况下块数据会存储3份，叫做三副本，副本块存储在不同服务器上 默认存储策略由BlockPlacementPolicyDefault类支持。也就是日常生活中提到最经典的3副本策略。 1st replica 如果写请求方所在机器是其中一个datanode,则直接存放在本地,否则随机在集群中选择一个datanode. 2nd replica 第二个副本存放于不同第一个副本的所在的机架 3rd replica 第三个副本存放于第二个副本所在的机架,但是属于不同的服务器节点 写入数据流程： 1）客户端向NameNode请求上传文件 ，NameNode检查目标文件是否已存在 ，父目录是否存在。 2）NameNode返回是否可以上传。 3）客户端请求第一个 block上传到哪几个datanode服务器上。 4）NameNode返回3个datanode节点 ，分别为dn1、dn2、dn3。 5）客户端请求dn1上传数据 ，dn1收到请求会继续调用dn2 ，然后dn2调用dn3 ，将这个通信管道建立完成。 6）dn1、dn2、dn3逐级应答客户端 7）客户端开始往dn1上传第一个block（先从磁盘读取数据放到一个本地内存缓存），以packet为单位 ，dn1 收到一个packet就会传给dn2 ，dn2传给dn3；dn1每传一个packet会放入一个应答队列等待应答 8）当一个block传输完成之后 ，客户端再次请求NameNode上传第二个block的服务器。（重复执行3-7步） 读取数据流程： 1）客户端向NameNode请求下载文件 ，NameNode通过查询元数据 ，找到文件块所在的datanode地址。 2）挑选一台datanode（就近原则 ，然后随机）服务器 ，请求读取数据。 3）datanode开始传输数据给客户端（从磁盘里面读取数据放入流 ，以packet为单位来做校验）。 4）客户端以packet为单位接收 ，先在本地缓存 ，然后写入目标文件。 2.Secondary NameNode工作机制 NameNode管理着元数据信息，其中有两类持久化元数据文件：edits操作日志文件和fsimage元数据镜像文件。新的操作日志不会立即与fsimage进行合并，也不会刷到NameNode的内存中，而是会先写到edits中(因为合并需要消耗大量的资源)，操作成功之后更新至内存 SecondaryNameNode有两个作用，一是镜像备份，二是日志与镜像的定期合并。两个过程同时进行，称为checkpoint（检查点）。 这个过程大体就是三个步骤：拷贝、合并、替换 fsimage是存储文件系统的快照（元数据镜像文件） fsimage文件其实是Hadoop文件系统元数据的一个永久性的检查点，其中包含Hadoop文件系统中的所有目录和文件idnode的序列化信息；fsimage包含Hadoop文件系统中的所有目录和文件idnode的序列化信息；对于文件来说，包含的信息有修改时间、访问时间、块大小和组成一个文件块信息等；而对于目录来说，包含的信息主要有修改时间、访问控制权限等信息。 edits记录对文件系统的更改，也就是编辑日志，存放的是Hadoop文件系统的所有更新操作的路径，文件系统客户端执行的所以写操作首先会被记录到edits文件中 checkpoint机制的触发条件： checkpoint机制是secondname和NameNode之间的数据操作 该机制决定了secondname什么时候进行元数据的持久化保存 条件一 距离上一次保存时间过去了1个小时 条件二 文件的事务操作(文件写入，文件修改，文件删除)达到了100万次 两个条件任意一个满足就执行checkpoint 第一阶段：NameNode启动 （1）第一次启动NameNode格式化后 ，创建fsimage和edits文件。如果不是第一次启动 ，直接加载编辑日志和镜像文件到内存。 （2）客户端对元数据进行增删改的请求 （3）NameNode记录操作日志 ，更新滚动日志。 （4）NameNode在内存中对数据进行增删改查 第二阶段：SecondaryNameNode工作： （1）Secondary NameNode询问NameNode是否需要checkpoint。直接带回NameNode是否检查结果。 （2）Secondary NameNode请求执行checkpoint。 （3）NameNode滚动正在写的edits日志 （4）将滚动前的编辑日志和镜像文件拷贝到Secondary NameNode （5）Secondary NameNode加载编辑日志和镜像文件到内存 ，并合并。 （6）生成新的镜像文件fsimage.chkpoint （7）拷贝fsimage.chkpoint到NameNode （8）NameNode将fsimage.chkpoint重新命名成fsimage 3.NameNode与SecondaryNameNode的区别与联系？ 机制流程同上 区别： NameNode负责管理整个文件系统的元数据，以及每一个路径（文件）所对应的数据块信息。 SecondaryNameNode主要用于定期合并命名空间镜像和命名空间镜像的编辑日志。 联系： SecondaryNameNode中保存了一份和NameNode一致的镜像文件（fsimage）和日志文件（edits）。 在主NameNode发生故障时（假设没有及时备份数据），可以从SecondaryNameNode恢复数据。 4.服役新数据节点和退役旧节点步骤 节点上线操作： 当要新上线数据节点的时候，需要把数据节点的名字追加在 dfs.hosts 文件中 （1）关闭新增节点的防火墙 （2）在 NameNode 节点的 hosts 文件中加入新增数据节点的 hostname （3）在每个新增数据节点的 hosts 文件中加入 NameNode 的 hostname （4）在 NameNode 节点上增加新增节点的 SSH 免密码登录的操作 （5）在 NameNode 节点上的 dfs.hosts 中追加上新增节点的 hostname, （6）在其他节点上执行刷新操作：hdfs dfsadmin -refreshNodes （7）在 NameNode 节点上，更改 slaves 文件，将要上线的数据节点 hostname 追加到 slaves 文件中 （8）启动 DataNode 节点 （9）查看 NameNode 的监控页面看是否有新增加的节点 节点下线操作： （1）修改&#x2F;conf&#x2F;hdfs-site.xml 文件 （2）确定需要下线的机器，dfs.osts.exclude 文件中配置好需要下架的机器，这个是阻止下架的机器去连接 NameNode。 （3）配置完成之后进行配置的刷新操作.&#x2F;bin&#x2F;hadoop dfsadmin -refreshNodes,这个操作的作用是在后台进行 block 块的移动。 （4）当执行三的命令完成之后，需要下架的机器就可以关闭了，可以查看现在集群上连接的节点，正在执行 Decommission，会显示：Decommission Status : Decommission in progress 执行完毕后，会显示：Decommission Status : Decommissionedasdad （5）机器下线完毕，将他们从excludes 文件中移除。 5.NameNode挂了怎么办 方法一：将SecondaryNameNode中数据拷贝到namenode存储数据的目录； 方法二：使用-importCheckpoint选项启动namenode守护进程 ，从而将SecondaryNameNode中数据拷贝到namenode目录中。 步骤如下： ​ 拿一台和原来机器一样的机器，包括配置和文件，一般来说最快的是拿你节点机器中的一台，立马能用（部分配置要改成NameNode的配置）创建一个空的文件夹，该文件夹就是配置文件中dfs.name.dir所指向的文件夹。拷贝你的secondary NameNode checkpoint出来的文件，到某个文件夹，该文件夹为fs.checkpoint.dir指向的文件夹执行命令bin&#x2F;hadoop namenode -importCheckpoint这样NameNode会读取checkpoint文件，保存到dfs.name.dir。但是如果你的dfs.name.dir包含合法的fsimage，是会执行失败的。因为NameNode会检查fs.checkpoint.dir目录下镜像的一致性，但是不会去改动它。 6.Hadoop的NameNode宕机怎么解决​ 如果只是节点挂了 ，重启即可 ，如果是机器挂了 ，重启机器后看节点是否能重启 ，不能重启就要找到原因修复了 先看宕机后的损失如何，如果是内存中的数据丢失，但磁盘数据还在，可以将 secondary namenode 的工作目录 copy 到 namenode 的工作目录中，恢复上一次 checkpoint 的数据，这样可以恢复大部分数据，但不能恢复所有数据，因为有些数据还没做 checkpoint。 也可以设置 namenode 的工作目录在多块磁盘上，那么 edits 日志文件就会同时写在多块磁盘上，如果一个磁盘坏了，那另一块磁盘上仍保存着数据。这两个磁盘是可以并发的，磁盘 IO 不会起冲突。 最终解决方案是在建立集群初期的时候建立 NameNode HA 高可用模式。 HA机制通过配置两个NameNode节点，一个作为Active NameNode，另一个作为Standby NameNode，共同承担元数据管理的职责。Active NameNode负责处理客户端的请求，而Standby NameNode则作为热备节点，实时同步Active NameNode的元数据，确保在Active NameNode发生故障时能够迅速接管其职责。 实现NameNode的HA机制，需要依赖一些关键组件和技术，如ZKFailoverController和Zookeeper集群。ZKFailoverController负责监控NameNode的健康状况，并在主NameNode故障时借助Zookeeper实现自动的主备选举和切换。Zookeeper集群则为主备切换控制器提供主备选举支持，确保切换过程的高可用性和可靠性。","categories":[{"name":"Hadoop面试题","slug":"Hadoop面试题","permalink":"https://walims.xyz/categories/Hadoop%E9%9D%A2%E8%AF%95%E9%A2%98/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://walims.xyz/tags/Hadoop/"},{"name":"面试题","slug":"面试题","permalink":"https://walims.xyz/tags/%E9%9D%A2%E8%AF%95%E9%A2%98/"}]}],"categories":[{"name":"Spark面试题","slug":"Spark面试题","permalink":"https://walims.xyz/categories/Spark%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"name":"算法","slug":"算法","permalink":"https://walims.xyz/categories/%E7%AE%97%E6%B3%95/"},{"name":"排序","slug":"算法/排序","permalink":"https://walims.xyz/categories/%E7%AE%97%E6%B3%95/%E6%8E%92%E5%BA%8F/"},{"name":"Hadoop面试题","slug":"Hadoop面试题","permalink":"https://walims.xyz/categories/Hadoop%E9%9D%A2%E8%AF%95%E9%A2%98/"}],"tags":[{"name":"面试题","slug":"面试题","permalink":"https://walims.xyz/tags/%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"name":"Spark","slug":"Spark","permalink":"https://walims.xyz/tags/Spark/"},{"name":"算法","slug":"算法","permalink":"https://walims.xyz/tags/%E7%AE%97%E6%B3%95/"},{"name":"排序","slug":"排序","permalink":"https://walims.xyz/tags/%E6%8E%92%E5%BA%8F/"},{"name":"Hadoop","slug":"Hadoop","permalink":"https://walims.xyz/tags/Hadoop/"}]}